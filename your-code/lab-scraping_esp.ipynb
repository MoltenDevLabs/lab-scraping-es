{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Web-Scraping-Lab\" data-toc-modified-id=\"Web-Scraping-Lab-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Web Scraping Lab</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Useful-Resources\" data-toc-modified-id=\"Useful-Resources-1.0.1\"><span class=\"toc-item-num\">1.0.1&nbsp;&nbsp;</span>Useful Resources</a></span><ul class=\"toc-item\"><li><span><a href=\"#First-of-all,-gathering-our-tools.\" data-toc-modified-id=\"First-of-all,-gathering-our-tools.-1.0.1.1\"><span class=\"toc-item-num\">1.0.1.1&nbsp;&nbsp;</span>First of all, gathering our tools.</a></span></li><li><span><a href=\"#Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:\" data-toc-modified-id=\"Challenge-1---Download,-parse-(using-BeautifulSoup),-and-print-the-content-from-the-Trending-Developers-page-from-GitHub:-1.0.1.2\"><span class=\"toc-item-num\">1.0.1.2&nbsp;&nbsp;</span>Challenge 1 - Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:</a></span></li><li><span><a href=\"#Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.\" data-toc-modified-id=\"Display-the-names-of-the-trending-developers-retrieved-in-the-previous-step.-1.0.1.3\"><span class=\"toc-item-num\">1.0.1.3&nbsp;&nbsp;</span>Display the names of the trending developers retrieved in the previous step.</a></span></li><li><span><a href=\"#Challenge-2---Display-the-trending-Python-repositories-in-GitHub\" data-toc-modified-id=\"Challenge-2---Display-the-trending-Python-repositories-in-GitHub-1.0.1.4\"><span class=\"toc-item-num\">1.0.1.4&nbsp;&nbsp;</span>Challenge 2 - Display the trending Python repositories in GitHub</a></span></li><li><span><a href=\"#Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page\" data-toc-modified-id=\"Challenge-3---Display-all-the-image-links-from-Walt-Disney-wikipedia-page-1.0.1.5\"><span class=\"toc-item-num\">1.0.1.5&nbsp;&nbsp;</span>Challenge 3 - Display all the image links from Walt Disney wikipedia page</a></span></li><li><span><a href=\"#Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.\" data-toc-modified-id=\"Challenge-4---Retrieve-all-links-to-pages-on-Wikipedia-that-refer-to-some-kind-of-Python.-1.0.1.6\"><span class=\"toc-item-num\">1.0.1.6&nbsp;&nbsp;</span>Challenge 4 - Retrieve all links to pages on Wikipedia that refer to some kind of Python.</a></span></li><li><span><a href=\"#Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point\" data-toc-modified-id=\"Challenge-5---Number-of-Titles-that-have-changed-in-the-United-States-Code-since-its-last-release-point-1.0.1.7\"><span class=\"toc-item-num\">1.0.1.7&nbsp;&nbsp;</span>Challenge 5 - Number of Titles that have changed in the United States Code since its last release point</a></span></li><li><span><a href=\"#Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names\" data-toc-modified-id=\"Challenge-6---A-Python-list-with-the-top-ten-FBI's-Most-Wanted-names-1.0.1.8\"><span class=\"toc-item-num\">1.0.1.8&nbsp;&nbsp;</span>Challenge 6 - A Python list with the top ten FBI's Most Wanted names</a></span></li><li><span><a href=\"#Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org\" data-toc-modified-id=\"Challenge-7---List-all-language-names-and-number-of-related-articles-in-the-order-they-appear-in-wikipedia.org-1.0.1.9\"><span class=\"toc-item-num\">1.0.1.9&nbsp;&nbsp;</span>Challenge 7 - List all language names and number of related articles in the order they appear in wikipedia.org</a></span></li><li><span><a href=\"#Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk\" data-toc-modified-id=\"Challenge-8---A-list-with-the-different-kind-of-datasets-available-in-data.gov.uk-1.0.1.10\"><span class=\"toc-item-num\">1.0.1.10&nbsp;&nbsp;</span>Challenge 8 - A list with the different kind of datasets available in data.gov.uk</a></span></li><li><span><a href=\"#Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe\" data-toc-modified-id=\"Challenge-9---Top-10-languages-by-number-of-native-speakers-stored-in-a-Pandas-Dataframe-1.0.1.11\"><span class=\"toc-item-num\">1.0.1.11&nbsp;&nbsp;</span>Challenge 9 - Top 10 languages by number of native speakers stored in a Pandas Dataframe</a></span></li></ul></li><li><span><a href=\"#Stepping-up-the-game\" data-toc-modified-id=\"Stepping-up-the-game-1.0.2\"><span class=\"toc-item-num\">1.0.2&nbsp;&nbsp;</span>Stepping up the game</a></span><ul class=\"toc-item\"><li><span><a href=\"#Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-10---The-20-latest-earthquakes-info-(date,-time,-latitude,-longitude-and-region-name)-by-the-EMSC-as-a-pandas-dataframe-1.0.2.1\"><span class=\"toc-item-num\">1.0.2.1&nbsp;&nbsp;</span>Challenge 10 - The 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe\" data-toc-modified-id=\"Challenge-11---IMDB's-Top-250-data-(movie-name,-Initial-release,-director-name-and-stars)-as-a-pandas-dataframe-1.0.2.2\"><span class=\"toc-item-num\">1.0.2.2&nbsp;&nbsp;</span>Challenge 11 - IMDB's Top 250 data (movie name, Initial release, director name and stars) as a pandas dataframe</a></span></li><li><span><a href=\"#Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-12---Movie-name,-year-and-a-brief-summary-of-the-top-10-random-movies-(IMDB)-as-a-pandas-dataframe.-1.0.2.3\"><span class=\"toc-item-num\">1.0.2.3&nbsp;&nbsp;</span>Challenge 12 - Movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe.</a></span></li><li><span><a href=\"#Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.\" data-toc-modified-id=\"Challenge-13---Find-the-live-weather-report-(temperature,-wind-speed,-description-and-weather)-of-a-given-city.-1.0.2.4\"><span class=\"toc-item-num\">1.0.2.4&nbsp;&nbsp;</span>Challenge 13 - Find the live weather report (temperature, wind speed, description and weather) of a given city.</a></span></li><li><span><a href=\"#Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.\" data-toc-modified-id=\"Challenge-14---Book-name,price-and-stock-availability-as-a-pandas-dataframe.-1.0.2.5\"><span class=\"toc-item-num\">1.0.2.5&nbsp;&nbsp;</span>Challenge 14 - Book name,price and stock availability as a pandas dataframe.</a></span></li></ul></li></ul></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laboratorio de Web Scraping\n",
    "\n",
    "Encontrarás en este cuaderno algunos ejercicios de web scraping para practicar tus habilidades de scraping usando `requests` y `Beautiful Soup`.\n",
    "\n",
    "**Consejos:**\n",
    "\n",
    "- Verifica el [código de estado de la respuesta](https://http.cat/) para cada solicitud para asegurarte de haber obtenido el contenido previsto.\n",
    "- Observa el código HTML en cada solicitud para entender el tipo de información que estás obteniendo y su formato.\n",
    "- Busca patrones en el texto de respuesta para extraer los datos/información solicitados en cada pregunta.\n",
    "- Visita cada URL y echa un vistazo a su fuente a través de Chrome DevTools. Necesitarás identificar las etiquetas HTML, nombres de clases especiales, etc., utilizados para el contenido HTML que se espera extraer.\n",
    "- Revisa los selectores CSS.\n",
    "\n",
    "### Recursos Útiles\n",
    "- Documentación de la [biblioteca Requests](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Doc de Beautiful Soup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Lista de códigos de estado HTTP](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [Conceptos básicos de HTML](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [Conceptos básicos de CSS](https://www.cssbasics.com/#page_start)\n",
    "\n",
    "#### Primero que todo, reuniendo nuestras herramientas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ **Nuevamente, recuerda limitar tu salida antes de la entrega para que tu código no se pierda en la salida.**\n",
    "\n",
    "#### Desafío 1 - Descargar, analizar (usando BeautifulSoup) e imprimir el contenido de la página de Desarrolladores en Tendencia de GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_html=requests.get(url).text\n",
    "soup = BeautifulSoup(github_html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Muestra los nombres de los desarrolladores en tendencia recuperados en el paso anterior.\n",
    "\n",
    "Tu salida debe ser una lista de Python con los nombres de los desarrolladores. Cada nombre no debe contener ninguna etiqueta HTML.\n",
    "\n",
    "**Instrucciones:**\n",
    "\n",
    "1. Descubre la etiqueta HTML y los nombres de clase usados para los nombres de los desarrolladores. Puedes lograr esto usando Chrome DevTools.\n",
    "\n",
    "1. Usa BeautifulSoup para extraer todos los elementos HTML que contienen los nombres de los desarrolladores.\n",
    "\n",
    "1. Utiliza técnicas de manipulación de cadenas para reemplazar espacios en blanco y saltos de línea (es decir, `\\n`) en el *texto* de cada elemento HTML. Usa una lista para almacenar los nombres limpios.\n",
    "\n",
    "1. Imprime la lista de nombres.\n",
    "\n",
    "Tu salida debería lucir como abajo (con nombres diferentes):\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alex Gaynor', 'koide3', 'Gabriel Luiz Freitas Almeida', 'Jason Jean', 'Emil Ernerfeldt', 'Ha Thach', 'Michael Davis', 'Assaf Elovic', 'Henrik Rydgård', 'James Newton-King', 'sobolevn', 'Ian Lance Taylor', 'Toby Chui', 'Gao Sun', 'Quinn Slack', 'Me No Dev', 'Vectorized', 'Mihail Stoykov', 'Saúl Ibarra Corretgé', 'Fredrik Averpil', 'Mattt', 'jxxghp', 'Simon Eskildsen', 'Jessica A. Nash', 'Daniel Borkmann']\n"
     ]
    }
   ],
   "source": [
    "def extraer_nombres(soup):\n",
    "  nombres = []\n",
    "  nombres_html = soup.find_all('h1', class_='h3 lh-condensed')\n",
    "  \n",
    "  for nombre_html in nombres_html:\n",
    "    nombre = nombre_html.get_text(strip=True)\n",
    "    nombres.append(nombre)\n",
    "  \n",
    "  return nombres\n",
    "\n",
    "nombres = extraer_nombres(soup)\n",
    "print(nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 2 - Mostrar los repositorios de Python en tendencia en GitHub\n",
    "\n",
    "Los pasos para resolver este problema son similares al anterior, excepto que necesitas encontrar los nombres de los repositorios en lugar de los nombres de los desarrolladores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url2 = 'https://github.com/trending/python?since=daily'\n",
    "datos2 = requests.get(f\"{url2}\").text\n",
    "soup2 = BeautifulSoup(datos2, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mem0ai /mem0', 'stanford-oval /storm', 'hiyouga /LLaMA-Factory', 'microsoft /promptflow', 'comfyanonymous /ComfyUI', 'getsentry /sentry', 'ansible /awx', 'Avaiga /taipy', 'UKPLab /sentence-transformers', 'prowler-cloud /prowler', 'openai /tiktoken', 'python /typeshed', 'MervinPraison /PraisonAI', 'python-poetry /poetry', '3b1b /manim', 'princeton-nlp /SWE-agent']\n"
     ]
    }
   ],
   "source": [
    "def extraer_repos(soup2):\n",
    "  repos = []\n",
    "  repos_html = soup2.find_all('h2', class_='h3 lh-condensed')\n",
    "  \n",
    "  for repo_html in repos_html:\n",
    "    repo = repo_html.get_text(strip=True)\n",
    "    repos.append(repo)\n",
    "  \n",
    "  return repos\n",
    "\n",
    "repos = extraer_repos(soup2)\n",
    "print(repos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 3 - Mostrar todos los enlaces de imágenes de la página de Wikipedia de Walt Disney"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url3 = 'https://en.wikipedia.org/wiki/Walt_Disney'\n",
    "disney = requests.get(f\"{url3}\").text\n",
    "soup3 = BeautifulSoup(disney, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Walt_Disney_1946.JPG', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Walt_Disney_1942_signature.svg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Walt_Disney_Birthplace_Exterior_Hermosa_Chicago_Illinois.jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Walt_Disney_envelope_ca._1921.jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Walt_Disney_Snow_white_1937_trailer_screenshot_(13).jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Disney_drawing_goofy.jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:WaltDisneyplansDisneylandDec1954.jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Walt_disney_portrait_right.jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Walt_Disney_Grave.JPG', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:DisneySchiphol1951.jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Disney1968.jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Disney_Oscar_1953_(cropped).jpg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Disneyland_Resort_logo.svg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Animation_disc.svg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Mickey_Mouse_colored_(head).svg', 'https://en.wikipedia.org/wiki/Walt_Disney/wiki/File:Blank_television_set.svg']\n"
     ]
    }
   ],
   "source": [
    "def extraer_waltImgs(soup3):\n",
    "  waltImgs = []\n",
    "  waltImgs_html = soup3.find_all('a', class_='mw-file-description')\n",
    "  \n",
    "  for waltImg_html in waltImgs_html:\n",
    "    waltImg_text = waltImg_html.get_text(strip=True)\n",
    "    \n",
    "    enlace = waltImg_html.get('href')\n",
    "    if enlace:\n",
    "      if not enlace.startswith('http'):\n",
    "        enlace = 'https://en.wikipedia.org/wiki/Walt_Disney' + enlace\n",
    "    waltImgs.append(enlace)\n",
    "  \n",
    "  return waltImgs\n",
    "\n",
    "waltImgs = extraer_waltImgs(soup3)\n",
    "print(waltImgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 4 - Recuperar todos los enlaces a páginas en Wikipedia que se refieren a algún tipo de Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "url4 ='https://en.wikipedia.org/wiki/Python' \n",
    "python = requests.get(f\"{url4}\").text\n",
    "soup4 = BeautifulSoup(python, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['https://en.wikipedia.org/wiki/Python#', 'https://en.wikipedia.org/wiki/Python#Snakes', 'https://en.wikipedia.org/wiki/Python#Computing', 'https://en.wikipedia.org/wiki/Python#People', 'https://en.wikipedia.org/wiki/Python#Roller_coasters', 'https://en.wikipedia.org/wiki/Python#Vehicles', 'https://en.wikipedia.org/wiki/Python#Weaponry', 'https://en.wikipedia.org/wiki/Python#Other_uses', 'https://en.wikipedia.org/wiki/Python#See_also']\n"
     ]
    }
   ],
   "source": [
    "def extraer_enlacesPython(soup4):\n",
    "  enlacesPython = []\n",
    "  enlacesPython_html = soup4.find_all('a', class_='vector-toc-link')\n",
    "  \n",
    "  for enlacePython_html in enlacesPython_html:\n",
    "    enlacePython_text = enlacePython_html.get_text(strip=True)\n",
    "    \n",
    "    enlace = enlacePython_html.get('href')\n",
    "    if enlace:\n",
    "      if not enlace.startswith('http'):\n",
    "        enlace = 'https://en.wikipedia.org/wiki/Python' + enlace\n",
    "      enlacesPython.append(enlace)\n",
    "  \n",
    "  return enlacesPython\n",
    "\n",
    "enlacesPython = extraer_enlacesPython(soup4)\n",
    "print(enlacesPython)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 5 - Número de Títulos que han cambiado en el Código de los Estados Unidos desde su último punto de lanzamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url5 = 'http://uscode.house.gov/download/download.shtml'\n",
    "usa = requests.get(f\"{url5}\").text\n",
    "soup5 = BeautifulSoup(usa, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "def extraer_titulos(soup5):\n",
    "  titulos = []\n",
    "  titulos_html = soup5.find_all('div', class_='usctitle')\n",
    "  \n",
    "  for titulo_html in titulos_html:\n",
    "    titulo = titulo_html.get_text(strip=True)\n",
    "    titulos.append(titulo)\n",
    "  \n",
    "  return len(titulos) # Se usa len() para extraer en numero de titulos, para hacer una lista con los titulos solo hay que quitar el metodo len()\n",
    "\n",
    "titulos = extraer_titulos(soup5)\n",
    "print(titulos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 6 - Una lista de Python con los diez nombres más buscados por el FBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url6 = 'https://www.fbi.gov/wanted/topten'\n",
    "wanted = requests.get(f\"{url6}\").text\n",
    "soup6 = BeautifulSoup(wanted, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "nombresFBI = []\n",
    "secciones = soup6.find_all('div', class_=\"query-results pat-pager\")\n",
    "\n",
    "for seccion in secciones:\n",
    "  articulos = seccion.find_all('ul')\n",
    "  for articulo in articulos:\n",
    "    listas = articulo.find_all('li')\n",
    "    for lista in listas:\n",
    "      tilulos = lista.find_all('h3')\n",
    "      for titulo in titulos:\n",
    "        nombres = titulo.find_all('a')\n",
    "        nombresFBI.append(nombres)\n",
    "        \n",
    "print(nombresFBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# List to hold the names\n",
    "nombresFBI = []\n",
    "\n",
    "# Find the div containing the wanted list entries\n",
    "secciones = soup6.find_all('div', class_=\"query-results\")\n",
    "\n",
    "# Iterate through the sections and extract the names\n",
    "for seccion in secciones:\n",
    "    # Find all articles within the section\n",
    "    articulos = seccion.find_all('article')\n",
    "    for articulo in articulos:\n",
    "        # Find the heading containing the name\n",
    "        titulo = articulo.find('h3')\n",
    "        if titulo:\n",
    "            # Extract the name from the link within the heading\n",
    "            nombre = titulo.find('a').text.strip()\n",
    "            nombresFBI.append(nombre)\n",
    "\n",
    "# Print the extracted names\n",
    "print(nombresFBI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (501926152.py, line 38)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[16], line 38\u001b[1;36m\u001b[0m\n\u001b[1;33m    return noticias\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "  noticias = [] # Contenedor de noticias\n",
    "  secciones = soup.find_all('section', class_='b-t') # Secciones de noticias a buscar\n",
    "  \n",
    "  for seccion in secciones: # Recorrer las secciones\n",
    "    articulos = seccion.find_all('article') # Sacar los articulos en la seccion\n",
    "    for articulo in articulos:\n",
    "      \n",
    "      # Extraer titulo\n",
    "      titulo_tag = articulo.find('h2', class_='c_t') # Extraer titulos\n",
    "      if titulo_tag: # Comprobación de que el titulo es valido y existe\n",
    "        titulo = titulo_tag.get_text(strip=True)\n",
    "      else:\n",
    "        titulo = 'Titulo no disponible en el parseo'\n",
    "        \n",
    "      # Extraer resumen\n",
    "      resumen_tag = articulo.find('p', class_='c_d')\n",
    "      if resumen_tag:\n",
    "        resumen = resumen_tag.get_text(strip=True)\n",
    "      else:\n",
    "        resumen = 'Resumen no disponible en el parseo'\n",
    "        \n",
    "      # Extraer enclaces\n",
    "      enlace_tag = articulo.find('a')\n",
    "      if enlace_tag:\n",
    "        enlace = enlace_tag['href']\n",
    "        if not enlace.startswith('http'):\n",
    "          enlace = 'https://elpais.com' + enlace\n",
    "      else:\n",
    "        enlace = 'Enlace no disponible en el parseo'\n",
    "        \n",
    "      # Añadir noticia al contenedor\n",
    "      noticias.append({\n",
    "        'titulo': titulo,\n",
    "        'resumen': resumen,\n",
    "        'enlace': enlace\n",
    "      })\n",
    "      \n",
    "  return noticias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nombresFBI_html = soup6.find_all('div', class_='query_results')\n",
    "nombresFBI_html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "def extraer_nombresFBI(soup6):\n",
    "  nombresFBI = []\n",
    "  nombresFBI_html = soup6.find_all('h3', class_='title')\n",
    "  \n",
    "  for nombreFBI_html in nombresFBI_html:\n",
    "    nombreFBI = nombreFBI_html.get_text(strip=True)\n",
    "    nombresFBI.append(nombreFBI)\n",
    "  \n",
    "  return nombresFBI\n",
    "\n",
    "nombresFBI = extraer_nombresFBI(soup6)\n",
    "print(nombresFBI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 7 - Listar todos los nombres de idiomas y el número de artículos relacionados en el orden en que aparecen en wikipedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url7 = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages = requests.get(f\"{url7}\").text\n",
    "soup7 = BeautifulSoup(languages, 'html.parser')\n",
    "#langlist = soup7.find_all(\"div\", {\"class\": f\"central-featured-lang\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['English6,847,000+articles', 'æ\\x97¥æ\\x9c¬èª\\x9e1,421,000+è¨\\x98äº\\x8b', 'Deutsch2.924.000+Artikel', 'Ð\\xa0Ñ\\x83Ñ\\x81Ñ\\x81ÐºÐ¸Ð¹1Â\\xa0987Â\\xa0000+Ñ\\x81Ñ\\x82Ð°Ñ\\x82ÐµÐ¹', 'EspaÃ±ol1.965.000+artÃ\\xadculos', 'FranÃ§ais2â\\x80¯621â\\x80¯000+articles', 'ä¸\\xadæ\\x96\\x871,429,000+æ\\x9d¡ç\\x9b® / æ¢\\x9dç\\x9b®', 'Italiano1.871.000+voci', 'Ù\\x81Ø§Ø±Ø³Û\\x8cÛ±Ù¬Û°Û°Û¶Ù¬Û°Û°Û°+Ù\\x85Ù\\x82Ø§Ù\\x84Ù\\x87', 'PortuguÃªs1.128.000+artigos']\n"
     ]
    }
   ],
   "source": [
    "def extraer_idiomas(soup7):\n",
    "  idiomas = []\n",
    "  idiomas_html = soup7.find_all(\"div\", {\"class\": f\"central-featured-lang\"})\n",
    "  \n",
    "  for idioma_html in idiomas_html:\n",
    "    idioma = idioma_html.get_text(strip=True)\n",
    "    idiomas.append(idioma)\n",
    "  \n",
    "  return idiomas\n",
    "\n",
    "idiomas = extraer_idiomas(soup7)\n",
    "print(idiomas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 8 - Una lista con los diferentes tipos de conjuntos de datos disponibles en data.gov.uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url8 = 'https://data.gov.uk/'\n",
    "dats = requests.get(f\"{url8}\")\n",
    "soup8 = BeautifulSoup(dats.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Business and economy', 'Crime and justice', 'Defence', 'Education', 'Environment', 'Government', 'Government spending', 'Health', 'Mapping', 'Society', 'Towns and cities', 'Transport', 'Digital service performance', 'Government reference data']\n"
     ]
    }
   ],
   "source": [
    "def extraer_conjuntos(soup8):\n",
    "  conjuntos = []\n",
    "  conjuntos_html = soup8.find_all('h3', class_='govuk-heading-s')\n",
    "  \n",
    "  for conjunto_html in conjuntos_html:\n",
    "    conjunto = conjunto_html.get_text(strip=True)\n",
    "    conjuntos.append(conjunto)\n",
    "  \n",
    "  return conjuntos\n",
    "\n",
    "conjuntos = extraer_conjuntos(soup8)\n",
    "print(conjuntos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafío 9 - Los 10 idiomas con más hablantes nativos almacenados en un DataFrame de Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url9 = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'\n",
    "tenlang = requests.get(url9)\n",
    "soup9 = BeautifulSoup(tenlang.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Idioma': 'Mandarin Chinese', 'n_hablantes': '941'}, {'Idioma': 'Spanish', 'n_hablantes': '486'}, {'Idioma': 'English', 'n_hablantes': '380'}, {'Idioma': 'Hindi', 'n_hablantes': '345'}, {'Idioma': 'Bengali', 'n_hablantes': '237'}, {'Idioma': 'Portuguese', 'n_hablantes': '236'}, {'Idioma': 'Russian', 'n_hablantes': '148'}, {'Idioma': 'Japanese', 'n_hablantes': '123'}, {'Idioma': 'Yue Chinese', 'n_hablantes': '86'}, {'Idioma': 'Vietnamese', 'n_hablantes': '85'}, {'Idioma': 'Turkish', 'n_hablantes': '84'}, {'Idioma': 'Wu Chinese', 'n_hablantes': '83'}, {'Idioma': 'Marathi', 'n_hablantes': '83'}, {'Idioma': 'Telugu', 'n_hablantes': '83'}, {'Idioma': 'Western Punjabi', 'n_hablantes': '82'}, {'Idioma': 'Korean', 'n_hablantes': '81'}, {'Idioma': 'Tamil', 'n_hablantes': '79'}, {'Idioma': 'Egyptian Arabic', 'n_hablantes': '78'}, {'Idioma': 'Standard German', 'n_hablantes': '76'}, {'Idioma': 'French', 'n_hablantes': '74'}, {'Idioma': 'Urdu', 'n_hablantes': '70'}, {'Idioma': 'Javanese', 'n_hablantes': '68'}, {'Idioma': 'Italian', 'n_hablantes': '64'}, {'Idioma': 'Iranian Persian', 'n_hablantes': '62'}, {'Idioma': 'Gujarati', 'n_hablantes': '58'}, {'Idioma': 'Hausa', 'n_hablantes': '54'}, {'Idioma': 'Bhojpuri', 'n_hablantes': '53'}, {'Idioma': 'Levantine Arabic', 'n_hablantes': '51'}, {'Idioma': 'Southern Min', 'n_hablantes': '51'}]\n"
     ]
    }
   ],
   "source": [
    "tabla = soup9.find('table', class_='wikitable')\n",
    "filas = tabla.find_all('tr')[1:]\n",
    "hablantes = []\n",
    "\n",
    "for fila in filas:\n",
    "  celdas = fila.find_all('td')\n",
    "  \n",
    "  idioma = celdas[0].text.strip()\n",
    "  n_hablantes = celdas[1].text.strip()\n",
    "  \n",
    "  dic = {\n",
    "  'Idioma': idioma,\n",
    "  'n_hablantes': n_hablantes,\n",
    "  }\n",
    "\n",
    "  hablantes.append(dic)\n",
    "  \n",
    "print(hablantes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subiendo el nivel\n",
    "### Desafío 10 - La información de los 20 últimos terremotos (fecha, hora, latitud, longitud y nombre de la región) por el EMSC como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "#url7 = 'https://www.emsc-csem.org/Earthquake/'\n",
    "url10 = \"https://www.emsc-csem.org/#2\"\n",
    "terr = requests.get(url10)\n",
    "soup10 = BeautifulSoup(terr.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<!DOCTYPE html>\n",
       "\n",
       "<html lang=\"en\"><head><meta charset=\"utf-8\"/><meta content=\"srFzNKBTd0FbRhtnzP--Tjxl01NfbscjYwkp4yOWuQY\" name=\"google-site-verification\"><meta content=\"BCAA3C04C41AE6E6AFAF117B9469C66F\" name=\"msvalidate.01\"/><meta content=\"43b36314ccb77957\" name=\"y_key\"/><meta content=\"all\" name=\"robots\"/><meta content=\"earthquakes today - recent and latest earthquakes, earthquake map and earthquake information. Earthquake information for europe. EMSC (European Mediterranean Seismological Centre) provides real time earthquake information for seismic events with magnitude larger than 5 in the European Mediterranean area and larger than 7 in the rest of the world.\" lang=\"en\" name=\"description\"/><meta content=\"705855916142039\" property=\"fb:app_id\"/><meta content=\"en_FR\" property=\"og:locale\"/><meta content=\"website\" property=\"og:type\"/><meta content=\"EMSC - European-Mediterranean Seismological Centre\" property=\"og:site_name\"/><meta content=\"//www.emsc-csem.org/\" property=\"og:url\"/><link href=\"/favicon.png\" rel=\"icon\" type=\"image/x-icon\"/>\n",
       "<title>EMSC - European-Mediterranean Seismological Centre</title>\n",
       "<script> console.log((new Date()).toString());</script><link href=\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.css\" rel=\"stylesheet\"/><link href=\"//static3.emsc.eu/Css/m_emsc.min.css?v=2\" rel=\"stylesheet\"/><script src=\"//static1.emsc.eu/javascript/jquery-3.6.0.min.js\"></script><script> var emsc_ws_url=\"wss://cobra.emsc-csem.org/home\";</script><script src=\"https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.js\"></script><script src=\"//static1.emsc.eu/javascript/home.min.js?v=2\"></script><script src=\"//static2.emsc.eu/javascript/src/home_mapmember.js\"></script><script src=\"//static2.emsc.eu/javascript/emsc.min.js\"></script><style>.bg-HOME{width:100px;height:97px;background:url(//static1.emsc.eu/Css/img/home_sprites.png);display:inline-block}.bg-HOME_app{background-position:-103px -1px}.bg-HOME_queries{background-position:-1px -1px}.bg-HOME_searching{background-position:-1px -100px}.bg-HOME_felt_button{width:30px;height:29px;background-position:-103px -100px}.hleft{width:35%;display:inline-block}.quacc{border-left:8px solid rgb(179,0,18);padding-left:10px;margin-bottom:60px}.qusea-icon{width:100px;height:100px;margin:auto}.qusearch{display:inline-block;width:32%;text-align:center;vertical-align:top}.qusearch a{color:rgb(179,0,18);text-decoration:none}.qusea-label{padding:10px 0}.quacclab{font-weight:bold;font-size:26px;padding-bottom:5px}.qacc{font-weight:bold}#map_emsc_members{width:100%;height:320px;border:1px solid grey}.quacc:last-child{padding-right:20px}.news a div{text-align:left}.news .qusea-date{color:rgb(166,166,166);font-size:14px;font-style:italic;margin:0 5px}.news .qusea-label{color:rgb(0,0,0);font-size:14px;margin:0 5px}.qusea-icon.intensmap{position:relative;overflow:hidden;width:90%}.qusea-icon.intensmap img{position:absolute;top:-9999px;bottom:-9999px;left:-9999px;right:-9999px;margin:auto}.hright{width:62%;display:inline-block;float:right}.hmap{height:300px;border:1px solid black;margin-bottom:20px}.home_t2{font-weight:bold;width:50%;font-size:20px;display:inline-block;vertical-align:middle}.t2_hfe{text-align:right}.hfe{display:inline-flex;vertical-align:middle;line-height:normal}.home_felt{padding-left:20px;color:rgb(179,0,18)}.htab{position:relative}.table-scroll tbody{position:absolute;overflow-y:scroll;overflow-x:hidden;height:600px;margin-right:-10px}.table-scroll tr{width:100%;table-layout:fixed;display:inline-table}.table-scroll thead>tr>th{border:0;border-bottom:2px solid}.table-scroll th.thico{border-bottom:0}.table-scroll thead>tr>th,.table-scroll tbody>tr>td{padding:15px 3px;overflow:hidden}.table-scroll tbody{scrollbar-color:#F08080 #eee;scrollbar-width:thin}.table-scroll tbody::-webkit-scrollbar{width:10px}.table-scroll tbody::-webkit-scrollbar-track{background-color:#eee}.table-scroll tbody::-webkit-scrollbar-thumb{background-color:#F08080;background-clip:content-box}.table-scroll tbody::-webkit-scrollbar-thumb:hover{background-color:#a8bbbf}.htab table{width:100%}.htab table.eqs td{padding:8px 3px;font-size:14px}.tbmag{text-align:center}.tblat,.tblon,.tbdep{text-align:right}.tbreg{width:40%;text-align:left;white-space:nowrap;overflow:hidden;text-overflow:ellipsis}.tbdat{width:20%;text-align:left}.tbdep{padding-right:15px!important}.lilist:hover,.lilist.hover{border:1px solid #eee;font-weight:bold;cursor:pointer}.tago{text-align:center;font-size:10px;color:grey}.leaf-evid-mag{font-weight:bold}.content{font-size:16px}.evtyp{width:26px;height:26px;background:url(//static2.emsc.eu/Css/img/sprites_eqtype.png);float:right;margin:1px;padding:0;display:inline-block}.bg-type_sonicboom{background-position:-29px -1px}.bg-type_volcano{background-position:-1px -1px}.bg-type_explosion{background-position:-113px -1px}.bg-type_landslide{background-position:-141px -1px}.bg-type_induced{background-position:-57px -1px}.bg-type_tsunami_pending{background-position:-169px -1px}.bg-type_tsunami{background-position:-197px -1px}.bg-type_tsunami_NO{background-position:-85px -1px}.htab{margin-left:-60px}.htab table.eqs td.tbico{padding:0;width:60px}.eqs thead>tr>th.thico{padding:0;margin:0;width:60px}.eqs{border-spacing:0}.eqs tbody{width:100%}.eqs tbody>tr{width:inherit;position:relative;display:table}.eqs thead>tr>th{padding:15px 1px;text-align:center}.eqs thead>tr>th.tbreg{text-align:left}.tbdat{text-align:center}.eqs tr:nth-child(2n){background:#ccc}.eqs td.tbico{background:white}.eqs th>div{font-size:small}.eqs tr.rw,.eqs tr.bow{font-weight:bold}.eqs tr.rw,tr.rw .tbdat a{color:red}.feltbt{background-color:rgb(179,0,18);padding:15px;border-radius:5px;cursor:pointer}.feltbt .home_felt{color:white}.ht20{font-size:26px}.lds-dual-ring{left:65%;margin-left:-40px;display:inline-block;width:80px;height:80px;position:absolute}.lds-dual-ring:after{content:\" \";display:block;width:64px;height:64px;margin:8px;border-radius:50%;border:6px solid #000;border-color:#000 transparent #000 transparent;animation:lds-dual-ring 1.2s linear infinite}@keyframes lds-dual-ring{0%{transform:rotate(0deg)}100%{transform:rotate(360deg)}}.tbdat a{text-decoration:none;color:black}th\n",
       "@media (max-width:1270px){.content{margin:20px 5%}.eqs{font-size:14px}}.citiz{width:80px}.dm{display:inline-block;width:50%;margin-top:3px;text-align:center}.dm.comm:after,.dm.pic:after{width:16px;height:16px;background:url(//static3.emsc.eu/Images/icon/sprite_com_pic.png);display:block;content:\"\";margin:auto}.dm.comm:after{background-position:-19px -1px}.dm.pic:after{background-position:-1px -1px}.htab table.eqs td.tdcom{padding-left:5px}.hright2{margin-top:610px}.leaflet-bar a{background-color:#fff;border-bottom:1px solid #ccc;color:#444;display:block;height:26px;width:26px;line-height:1.45!important;text-align:center;text-decoration:none;font:bold 22px \"Lucida Console\",Monaco,monospace}.legend{box-sizing:border-box;border:2px solid #8c8c8c;padding:10px;text-align:left;margin:0 auto;background-color:white}.legend .title{text-align:center;font-size:16px;font-weight:bold}.dot{height:8px;width:8px;border-radius:50%;display:inline-block;vertical-align:middle}.dot.active{background:rgba(179,0,18,.4);border:4px solid rgba(179,0,18,1.00)}.dot.rights{background:rgba(231,167,167,.4);border:4px solid rgba(231,167,167,1.00)}.dot.nodal{background:rgba(0,0,0,.4);border:4px solid rgba(0,0,0,1.00)}.square_icon_active{width:20px;height:20px;background-color:rgba(179,0,18,.40);border:2px solid rgba(179,0,18,1.00)}.square_icon_nodal{width:20px;height:20px;background-color:rgba(0,0,0,.40);border:2px solid rgba(0,0,0,1.00)}.square_icon_rights{width:20px;height:20px;background-color:rgba(231,167,167,.4);;border:2px solid rgba(231,167,167,1.00)}.box{display:inline-block;margin:0 5px}.box:before{content:\"\";width:10px;height:10px;vertical-align:center;margin:0 4px -10%;display:inline-block}.rights:before{background-color:rgba(231,167,167,.4);;border:2px solid rgba(231,167,167,1.00)}.active:before{background-color:rgba(179,0,18,.40);border:2px solid rgba(179,0,18,1.00)}.nodal:before{background-color:rgba(0,0,0,.40);border:2px solid rgba(0,0,0,1.00)}</style></meta></head><body><div class=\"banner\" role=\"banner\"><div class=\"banner-ct\"><div class=\"bann-logo\">\n",
       "<a href=\"/\">\n",
       "<div class=\"spe emsc-logo\"></div>\n",
       "<div class=\"emsc-logo-label\">Centre Sismologique Euro-Méditerranéen</div>\n",
       "<div class=\"emsc-logo-label\">Euro-Mediterranean Seismological Centre</div>\n",
       "</a>\n",
       "</div><div class=\"hmenu\"><div class=\"hmenu0 hmenu1\"><div class=\"hmenus menut mt1\">Earthquakes</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Earthquake_map/\">World map</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Earthquake_information/\">Latest earthquakes</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Earthquake_data/\">Seismic data</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/Special_reports/\">Special reports</a></div></div><div class=\"hmenu0 hmenu2\"><div class=\"hmenus menut mt2\">LastQuake</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/lastquake/how_it_works/\">How it works</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/lastquake/information_channels/\">Information channels</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/lastquake/citizen_seismology/\">Citizen seismology</a></div></div><div class=\"hmenu0 hmenu3\"><div class=\"hmenus menut mt3\">About Us</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/about_us/who_we_are/\">Who we are</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/about_us/what_we_do/\">What we do</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/about_us/timeline/\">Timeline</a></div></div><div class=\"hmenu0 hmenu4\"><div class=\"hmenus menut mt4\">Partner with us</div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/partner_with_us/mission_and_vision/\">Mission &amp; vision</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/partner_with_us/partners/\">Partners</a></div><div class=\"hmenus menus\"><a href=\"//www.emsc-csem.org/partner_with_us/support_our_work/\">Support our work</a></div></div></div><div class=\"btncont\"><a class=\"hbt hbtdonate\" href=\"/donate/\">Donate</a><a class=\"hbt hbtlogin\" href=\"/Member/login.php\">Log in</a></div></div>\n",
       "<div class=\"emsctime\"></div>\n",
       "</div><div class=\"bandeau\"><div class=\"bandeaumv\"></div></div><div class=\"content\" role=\"main\"><div class=\"hleft\"><div class=\"quacc\">\n",
       "<div class=\"quacclab\">🎉 50<sup>th</sup> anniversary 🎉</div>\n",
       "<p>\n",
       "\t\t Join us in celebrating our 50<sup>th</sup> anniversary by leaving your testimony in our <a href=\"/about_us/guestbook\">guestbook</a>. \n",
       "        </p>\n",
       "</div><div class=\"quacc\">\n",
       "<div class=\"quacclab\">New Activity Report</div>\n",
       "<p>\n",
       "\t\tDiscover the achievements and milestones of the past year in our <a href=\"/Doc/EMSC_DOCS/EMSC_RT_activities_2023.pdf\" target=\"_blank\"> EMSC 2023 Activity Report</a>. \n",
       "\t\t</p>\n",
       "</div><div class=\"quacc\">\n",
       "<div class=\"quacclab\">Quick access</div>\n",
       "<div class=\"qusearch\"><a href=\"/Earthquake_information/\"><div class=\"qusea-icon\"><span class=\"bg-HOME bg-HOME_searching\"></span></div><div class=\"qusea-label qacc\">Search for earthquakes</div></a></div>\n",
       "<div class=\"qusearch\"><a href=\"/Earthquake_data/Data_queries.php\"><div class=\"qusea-icon\"><span class=\"bg-HOME bg-HOME_queries\"></span></div><div class=\"qusea-label qacc\">Data queries</div></a></div>\n",
       "<div class=\"qusearch\"><a href=\"/lastquake/information_channels/lastquake_app/\"><div class=\"qusea-icon\"><span class=\"bg-HOME bg-HOME_app\"></span></div><div class=\"qusea-label qacc\">Android &amp; iOS app</div></a></div>\n",
       "</div><div class=\"quacc\">\n",
       "<div class=\"quacclab\">Earthquake news</div><div class=\"qusearch news\"><a href=\"/Earthquake_information/earthquake.php?id=1680858\"><div class=\"qusea-icon intensmap\"><img alt=\"intensity map\" src=\"/Images/FELTREPORTS/168/1680858/IntensityMap.png\"/></div><div class=\"qusea-date\">Mon, 8 Jul 2024 09:54</div><div class=\"qusea-label\">Magnitude 5.3 earthquake in  La Concepción, Panama</div></a></div><div class=\"qusearch news\"><a href=\"/Earthquake_information/earthquake.php?id=1679229\"><div class=\"qusea-icon intensmap\"><img alt=\"intensity map\" src=\"/Images/FELTREPORTS/167/1679229/IntensityMap.png\"/></div><div class=\"qusea-date\">Thu, 4 Jul 2024 03:12</div><div class=\"qusea-label\">Magnitude 5.1 earthquake in  Katsuura, Japan</div></a></div><div class=\"qusearch news\"><a href=\"/Earthquake_information/earthquake.php?id=1677947\"><div class=\"qusea-icon intensmap\"><img alt=\"intensity map\" src=\"/Images/FELTREPORTS/167/1677947/IntensityMap.png\"/></div><div class=\"qusea-date\">Sun, 30 Jun 2024 20:57</div><div class=\"qusea-label\">Magnitude 4.9 earthquake in  Sarpol-e Z̄ahāb, Iran</div></a></div></div><script> var memb = {\"categories\":{\"nodal\":\"Key nodal members\",\"rights\":\"Members by right\",\"active\":\"Active members\"},\"lists\":{\"active\":{\"41.3275_19.81889\":[\"IGEO: Institute of Geosciences  (Tirana - Albania) - Contact: Dr. E. DUSHI\"],\"36.73225_3.08746\":[\"CRAAG: Centre de Recherche en Astronomie, Astrophysique et Geophysique  (Algiers - Algeria) - Contact: Dr. H. BELDJOUDI\"],\"40.18111_44.51361\":[\"NSSP: National Survey for Seismic Protection  (Yerevan - Armenia) - Contact: Dr. S. MARGARYAN\"],\"48.20849_16.37208\":[\"GSA: GeoSphere Austria (Vienna - Austria) - Contact: DI Helmut HAUSMANN \"],\"40.366656_49.835183\":[\"RSSC: Republican Seismic Survey Center of Azerbaijan National Academy of Sciences (Baku - Azerbaijan) - Contact: Dr. G.J.YETIRMISHLI\"],\"53.9_27.56667\":[\"CGM: Center of Geophysical Monitoring  (Minsk - Belarus) - Contact: \\tDr. V. ARONOV\"],\"50.85045_4.34878\":[\"ORB\\/ROB: Royal Observatory of Belgium  (Brussels - Belgium) - Contact: Dr. F. COLLIN\"],\"43.84864_18.35644\":[\"FMI: Federal Meteorological Institute  (Sarajevo - Bosnia and Herzegovina) - Contact: \\tM. GENJAC\"],\"44.77842_17.19386\":[\"RHI: Republic Hydrometeorological Institute  (Banja Luka - Bosnia and Herzegovina) - Contact: \\tDr. S. CVIJIC-AMULIC\"],\"42.69751_23.32415\":[\"NIGGG: National Institute in Geophysics, Geodesy and Geography - BAS (Sofia - Bulgaria) - Contact: \\tAss. Prof. PLAMENA RAYKOVA\"],\"45.81444_15.97798\":[\"AMGI & CSS: Andrija Mohorovicic Geophysical Institute and Croatian Seismological Survey  (Zagreb - Croatia) - Contact: \\tI. IVANCIC\"],\"35.17531_33.3642\":[\"GSD: Geological Survey Department  (Nicosia - Cyprus) - Contact: C. CHATZIGEORGIOU\"],\"49.19522_16.60796\":[\"IPE: Institute of Physics of the Earth, Brno  (Brno - Czech Republic) - Contact: \\tDr. P. SPACEK\"],\"50.08804_14.42076\":[\"GFU: Geophysical Institute of the Academy of Sciences  (Prague - Czech Republic) - Contact: \\tDr. J. ZEDNIK\"],\"55.67594_12.56553\":[\"GEUS: Geological Survey of Denmark and Greenland  (Copenhagen - Denmark) - Contact: Dr. P. VOSS\"],\"11.58901_43.14503\":[\"CERD: Observatoire Geophysique d'Arta  (Djibouti - Djibouti) - Contact: SAAD IBRAHIM\"],\"30.06263_31.24967\":[\"NRIAG: National Research Institute of Astronomy and Geophysics  (Cairo - Egypt) - Contact: Prof. Sherif Elhady\"],\"60.16952_24.93545\":[\"ISF: Institute of Seismology (Helsinki - Finland) - Contact: Dr T. TIIRA\"],\"45.17869_5.71479\":[\"ISTerre: ISTerre, Institut des Sciences de la Terre (Grenoble - France) - Contact: \\tDr. P. GUEGUEN\"],\"48.58392_7.74553\":[\"BCSF: Bureau Central Sismologique Francais  (Strasbourg - France) - Contact: Dr. F. MASSON\"],\"41.69411_44.83368\":[\"SMC: Seismic Monitoring Centre of Georgia  (Tbilisi - Georgia) - Contact: \\tDr. T. GODOLADZE\"],\"52.52437_13.41053\":[\"BGR: Federal Institute for Geosciences and Natural Resources  (Berlin - Germany) - Contact: Dr. K. STAMMLER\"],\"37.98376_23.72784\":[\"NOA: National Observatory of Athens (Athens - Greece) - Contact: \\tDr. A. TSELENTIS\"],\"38.24444_21.73444\":[\"UPSL: Laboratory of Seismology, University of Patras (Patra - Greece) - Contact: \\tDr. E. SOKOS\"],\"40.64361_22.93086\":[\"ITSAK: Institute of Engineering Seismology and Earthquake Engineering  (Thessaloniki - Greece) - Contact: Dr. C. PAPAIOANNOU\",\"AUTH: University of Thessaloniki  (Thessaloniki - Greece) - Contact: \\tDr. E. SCORDILIS\"],\"47.49835_19.04045\":[\"BUD: HUN-REN EPSS K\\u00f6vesligethy Rad\\u00f3 Seismological Observatory (Budapest - Hungary) - Contact: B\\u00e1lint S\\u00fcle\"],\"64.13548_-21.89541\":[\"IMO: Icelandic Meteorological Office  (Reykjav\\u00edk - Iceland) - Contact: \\tDr. G. GUDMUNDSSON\"],\"53.33306_-6.24889\":[\"DIAS: Dublin Institute for Advanced Studies  (Dublin - Ireland) - Contact: Dr. Martin M\\u00f6llhoff\"],\"31.87808_34.73983\":[\"NDC: National Data Center of Israel, Soreq Nuclear Research Center (Yavne - Israel) - Contact: Dr. Y. RADZYNER\"],\"31.76904_35.21633\":[\"GSI: Geological Survey of Israel  (Jerusalem - Israel) - Contact: \\tDr Ran NOF\"],\"45.64953_13.77678\":[\"OGS: Istituto Nazionale di Oceanografia e Geofisica Sperimentale (Trieste - Italy) - Contact: Pr. S. PAROLAI\"],\"31.95522_35.94503\":[\"JSO: Jordan Seismological Observatory (Amman - Jordan) - Contact: Ghassan SWEIDAN\"],\"42.67272_21.16688\":[\"GSK: Seismological Institute of Kosovo (Pristina - Kosovo) - Contact: \\tM. M. SHEMSI\"],\"33.89332_35.50157\":[\"SGB: Geophysics Centre at Bhannes  (Beirut - Lebanon) - Contact: \\tDr. M. BRAX\"],\"32.88743_13.18733\":[\"LCRSSS: Libyan Center for Remote Sensing and Space Science  (Tripoli - Libya) - Contact: Dr. A. ELMELADE\"],\"49.61167_6.13\":[\"ECGS: European Center for Geodynamics and Seismology  (Luxembourg - Luxembourg) - Contact: Dr. A. OTH\"],\"35.89968_14.5148\":[\"UM: Department, University of Malta  (Valletta - Malta) - Contact: \\tDr. P. GALEA\"],\"47.00556_28.8575\":[\"ASM-CIP: Academy of Sciences of Republic of Moldova (Chisinau - Moldova) - Contact: Dr. I. NICOARA\"],\"43.73333_7.41667\":[\"Direction de l'Environnement (Monaco - Monaco) - Contact: Ms J. ASTIER\"],\"42.44111_19.26361\":[\"MSO: Institute of Hydrometeorology and Seismology  (Podgorica - Montenegro) - Contact: MSc J. MIHALJEVIC\"],\"34.01325_-6.83255\":[\"D\\u00e9partement des Sciences de la Terre (Rabat - Morocco) - Contact: Dr. Y. TIMOULALI\",\"CNRST: Centre National pour la Recherche Scientifique et Technique  (Rabat - Morocco) - Contact: J. NACER\"],\"52.37403_4.88969\":[\"KNMI: Royal Netherlands Meteorological Institute  (Amsterdam - Netherlands) - Contact: Dr. R. SLEEMAN\"],\"41.99646_21.43141\":[\"SKO: Seismological Observatory (Skopje - North Macedonia) - Contact: Dr. D. CERNIH\"],\"59.95597_11.04918\":[\"NORSAR: Norwegian Seismic Array  (Lillestrom - Norway) - Contact: \\tDr. J. SCHWEITZER\"],\"60.39299_5.32415\":[\"BER: University of Bergen  (Bergen - Norway) - Contact: \\tDr. L. OTTEMOLLER\"],\"52.22977_21.01178\":[\"IGPAS: Institute of Geophysics, Polish Academy of Sciences  (Warsaw - Poland) - Contact: L. RUDZINSKI\"],\"38.71667_-9.13333\":[\"Faculdade de Ci\\u00eancias da Universidade de Lisboa (Lisbon - Portugal) - Contact: C. CORELA\",\"IMP: Instituto de Meteorologia  (Lisbon - Portugal) - Contact: Dr. F. CARRILHO\"],\"38.56667_-7.9\":[\"Universidade de Evora (\\u00c9vora - Portugal) - Contact: Dr. M. BEZZEGHOUD\"],\"44.43225_26.10626\":[\"NIEP: National Institute for Earth Physics  (Bucharest - Romania) - Contact: Dr. C. IONESCU\"],\"55.75222_37.61556\":[\"GSRAS: Geophysical Survey of the Russian Academy of Sciences  (Moscow - Russia) - Contact: Dr. A. MALOVICHKO\"],\"44.80401_20.46513\":[\"SSS: Seismological Survey of Serbia  (Belgrade - Serbia) - Contact: DEJAN VALCIC\"],\"48.14816_17.10674\":[\"ESI SAS: Earth Science Institute, SAS, Department of Seismology (Bratislava - Slovakia) - Contact: Dr. A. CIPCIAR\"],\"46.05108_14.50513\":[\"ARSO: Agencija Republike Slovenije za okolje  (Ljubljana - Slovenia) - Contact: \\tDr. I. CECIC\"],\"41.38879_2.15899\":[\"ICGC: Institut Cartografic i Geologic de Catalunya  (Barcelona - Spain) - Contact: \\tJ. ANTONIO JARA\"],\"59.85882_17.63889\":[\"SNSN: Swedish National Seismic Network  (Uppsala - Sweden) - Contact: B. LUND\"],\"47.36667_8.55\":[\"ETH: Schweizerischer Erdbebendienst  (Zurich - Switzerland) - Contact: Dr. J. Clinton\"],\"36.81897_10.16579\":[\"INMT: Institut National de la M\\u00e9t\\u00e9orologie  (Tunis - Tunisia) - Contact: Dr. S. B. ABDALLAH\"],\"41.01384_28.94966\":[\"KOERI: Kandilli Observatory and Earthquake Research Institute  (Istanbul - Turkey) - Contact: Prof. H. OZENER\"],\"39.91987_32.85427\":[\"ERD: Disaster and Emergency Management Presidency, Earthquake Department  (Ankara - Turkey) - Contact: Fatih ALVER\"],\"50.45466_30.5238\":[\"MCSM: Main Centre for Special Monitoring  (Kyiv - Ukraine) - Contact: M. A. LIASHCHUK\"],\"24.45118_54.39696\":[\"Dubai Municipality (Abu Dhabi - United Arab Emirates) - Contact: Mrs. E. A. AL KHATIBI\"],\"55.9125357_-3.3151365\":[\"BGS: British Geological Survey  (Edinburgh - United Kingdom) - Contact: M. SEGOU\"],\"15.35472_44.20667\":[\"NSOC: National Seismological Observatory Centre  (Sanaa - Yemen) - Contact: Dr. J. M. SHOLAN\"]},\"nodal\":{\"48.69572_2.18727\":[\"LDG: Laboratoire de D\\u00e9tection et de G\\u00e9ophysique  (Arpajon - France) - Contact: Dr. H. HEBERT\"],\"52.24437_13.041053\":[\"GFZ: GeoForschungsZentrum  (Berlin - Germany) - Contact: Dr. J. SAUL\"],\"45.46427_9.18951\":[\"INGV: Istituto Nazionale di Geofisica e Vulcanologia  (Milan - Italy) - Contact: Dr. M. LOCATI\"],\"41.89193_12.51133\":[\"INGV: Istituto Nazionale di Geofisica e Vulcanologia  (Rome - Italy) - Contact: L. SCOGNAMIGLIO\"],\"40.4165_-3.70256\":[\"IGN: Instituto Geografico Nacional  (Madrid - Spain) - Contact: J. V. CANTAVELLO NADAL\"]},\"rights\":{\"52.09083_5.12222\":[\"ORFEUS: Observatories and Research Facilities for EUropean Seismology  (Utrecht - Netherlands) - Contact: C. CAUZZI\"],\"46.20222_6.14569\":[\"ESC: European Seismological Commission  (Geneve - Switzerland) - Contact: Dr. A. OTH\"],\"51.3960409_-1.2377428\":[\"ISC: International Seismological Centre  (Thatcham - United Kingdom) - Contact: Dr. D. STORCHAK\"],\"38.89511_-77.03637\":[\"USGS: U.S. Geological Survey  (Washington - United States) - Contact: \\tM. P. EARLE\"]}}}; </script><div class=\"quacc\">\n",
       "<div class=\"quacclab\">EMSC members</div>\n",
       "<div>The EMSC is an international, non-profit NGO composed of more than 70 member institutes around the world. We operate a system for rapid collection, determination, and dissemination of earthquake parameters using seismic data contributed by seismological institutes together with crowdsourced data from earthquake eyewitnesses. Learn more about  \n",
       "        <a href=\"/about_us/who_we_are/\" rel=\"noopener\" style=\"text-decoration: underline;\" target=\"_blank\">who we are</a> and <a href=\"/about_us/what_we_do/\" rel=\"noopener\" style=\"text-decoration: underline;\" target=\"_blank\">what we do</a>. </div>\n",
       "<br/>\n",
       "<div id=\"map_emsc_members\"></div>\n",
       "</div></div><div class=\"hright\"><div class=\"hmap\" id=\"hmap\"></div><div class=\"home_t2 ht20\">Latest earthquakes</div><div class=\"home_t2 t2_hfe\"><span class=\"feltbt\"><span class=\"hfe\"><span class=\"bg-HOME bg-HOME_felt_button\"></span></span>\n",
       "<span class=\"hfe home_felt\">I felt an earthquake</span></span></div><div class=\"htab\"><table class=\"eqs table-scroll\">\n",
       "<thead><tr><th class=\"thico\"></th><th class=\"citiz\" colspan=\"2\"><div>Citizen<br/>response</div><div><div class=\"dm comm\"></div><div class=\"dm pic\"></div></div></th>\n",
       "<th class=\"tbdat\">Date &amp; Time<div>UTC</div></th><th class=\"tblat\">Lat.<div>degrees</div></th><th class=\"tblon\">Lon.<div>degrees</div></th><th class=\"tbdep\">Depth<div>km</div></th><th class=\"tbmag\">Mag.</th><th class=\"tbreg\">Region</th></tr></thead>\n",
       "<tbody></tbody>\n",
       "</table>\n",
       "</div><div class=\"hright2\"><b>Bold : Earthquakes with a magnitude ≥ 4.5 in Euro-med, or ≥ 5.5 in the world </b><br><b style=\"color:red;\">Red : Earthquakes with a magnitude ≥ 5.0 in Euro-med, or ≥ 6.0 in the world </b><br/></br></div></div><div style=\"clear:both;\"></div><script> console.log(\"TimeLoad\",0.004760980606); </script></div><div class=\"footer\"><div class=\"foot-cont\"><div class=\"part\"><div class=\"foot-logo-label\">EMSC is the European infrastructure for seismological products in</div><a aria-label=\"epos\" href=\"https://www.epos-eu.org/\" target=\"_blank\"><div class=\"spe foot-logo\"></div></a></div><div class=\"part2\"><div class=\"part-middle\"><a href=\"/faq/\">FAQ</a><a class=\"privacy\" href=\"/privacy/index.php\">© 2023 - privacy</a><a href=\"/contact/\">Contact us</a></div></div><div class=\"part p-soc\"><a aria-label=\"facebook EMSC.CSEM\" href=\"https://www.facebook.com/EMSC.CSEM/\" target=\"_blank\"><span class=\"spe f-facebook\"></span></a><a aria-label=\"twitter lastquake\" href=\"https://twitter.com/lastquake\" target=\"_blank\"><span class=\"spe f-twitter\"></span></a><a aria-label=\"linkedin emsc-csem\" href=\"https://www.linkedin.com/company/emsc-csem/\" target=\"_blank\"><span class=\"spe f-linkedin\"></span></a><a aria-label=\"youtube EuroMSC\" href=\"https://www.youtube.com/user/EuroMSC\" target=\"_blank\"><span class=\"spe f-youtube\"></span></a></div></div></div></body></html>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<table class=\"eqs table-scroll\">\n",
       "<thead><tr><th class=\"thico\"></th><th class=\"citiz\" colspan=\"2\"><div>Citizen<br/>response</div><div><div class=\"dm comm\"></div><div class=\"dm pic\"></div></div></th>\n",
       "<th class=\"tbdat\">Date &amp; Time<div>UTC</div></th><th class=\"tblat\">Lat.<div>degrees</div></th><th class=\"tblon\">Lon.<div>degrees</div></th><th class=\"tbdep\">Depth<div>km</div></th><th class=\"tbmag\">Mag.</th><th class=\"tbreg\">Region</th></tr></thead>\n",
       "<tbody></tbody>\n",
       "</table>"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tabla = soup10.find('table', class_='eqs')\n",
    "tabla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tbody></tbody>]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filas = tabla.find_all('tbody')\n",
    "filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fila \u001b[38;5;129;01min\u001b[39;00m filas:\n\u001b[0;32m      6\u001b[0m   celdas \u001b[38;5;241m=\u001b[39m fila\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtd\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 8\u001b[0m   fecha \u001b[38;5;241m=\u001b[39m \u001b[43mceldas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      9\u001b[0m   lat \u001b[38;5;241m=\u001b[39m celdas[\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m     10\u001b[0m   lon \u001b[38;5;241m=\u001b[39m celdas[\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mstrip()\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tabla = soup10.find('table', class_='eqs')\n",
    "filas = tabla.find_all('tr')\n",
    "terremotos = []\n",
    "\n",
    "for fila in filas:\n",
    "  celdas = fila.find_all('td')\n",
    "  \n",
    "  fecha = celdas[3].text.strip()\n",
    "  lat = celdas[4].text.strip()\n",
    "  lon = celdas[5].text.strip()\n",
    "  region = celdas[8].text.strip()\n",
    "  \n",
    "  dic = {\n",
    "  'fecha': fecha,\n",
    "  'lat': lat,\n",
    "  'lon': lon,\n",
    "  'region': region\n",
    "  }\n",
    "\n",
    "  terremotos.append(dic)\n",
    "  \n",
    "print(terremotos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desafío 11 - Datos del Top 250 de IMDB (nombre de la película, lanzamiento inicial, nombre del director y estrellas) como un dataframe de pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url11 = 'https://www.imdb.com/chart/top/'\n",
    "pelis = requests.get(url11)\n",
    "soup11 = BeautifulSoup(pelis.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head><title>403 Forbidden</title></head>\n",
       "<body>\n",
       "<center><h1>403 Forbidden</h1></center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titulo = soup11.find('h3', class_='ipc.title__text')\n",
    "titulo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find_all'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[73], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tabla \u001b[38;5;241m=\u001b[39m soup11\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mul\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mipc-metadata-list\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m filas \u001b[38;5;241m=\u001b[39m \u001b[43mtabla\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mli\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      3\u001b[0m peliculas \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fila \u001b[38;5;129;01min\u001b[39;00m filas:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find_all'"
     ]
    }
   ],
   "source": [
    "tabla = soup11.find('ul', class_='ipc-metadata-list')\n",
    "filas = tabla.find_all('li')[1:]\n",
    "peliculas = []\n",
    "\n",
    "for fila in filas:\n",
    "  celdas = fila.find_all('li')\n",
    "  \n",
    "  nombre = celdas[0].text.strip()\n",
    "  lanzamiento = celdas[1].text.strip()\n",
    "  director = celdas[2].text.strip()\n",
    "  estrellas = celdas[3].text.strip()\n",
    "  \n",
    "  dic = {\n",
    "  'nombre': nombre,\n",
    "  'lanzamiento': lanzamiento,\n",
    "  'director': director,\n",
    "  'estrellas': estrellas\n",
    "  }\n",
    "\n",
    "  peliculas.append(dic)\n",
    "  \n",
    "print(peliculas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 12 - Nombre de la película, año y un breve resumen de las 10 películas aleatorias top (IMDB) como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url12 = 'http://www.imdb.com/chart/top'\n",
    "pelis2 = requests.get(url12)\n",
    "soup12 = BeautifulSoup(pelis2.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head><title>403 Forbidden</title></head>\n",
       "<body>\n",
       "<center><h1>403 Forbidden</h1></center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = list(soup12.find_all(\"span\", {\"class\": \"secondaryInfo\"}))\n",
    "years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tu código aquí"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 13 - Encontrar el reporte meteorológico en vivo (temperatura, velocidad del viento, descripción y clima) de una ciudad dada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://openweathermap.org/current\n",
    "url13 = 'https://openweathermap.org/current'\n",
    "report = requests.get(url13)\n",
    "soup13 = BeautifulSoup(report.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte = soup13.find('h3', class_='ipc.title__text')\n",
    "reporte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'city': 'Eixample', 'temperature': 27.79, 'wind_speed': 1.03, 'weather': 'clear sky'}\n"
     ]
    }
   ],
   "source": [
    "def weather(lat, lon):\n",
    "  api_key = '9d8513551e11f6c454eb7682eacff771'\n",
    "  url = f\"https://api.openweathermap.org/data/2.5/weather?lat={lat}&lon={lon}&appid={api_key}&units=metric\"\n",
    "  response = requests.get(url)\n",
    "  data = response.json()\n",
    "  \n",
    "  weather_info = {\n",
    "  \"city\": data.get(\"name\"),\n",
    "  \"temperature\": data[\"main\"][\"temp\"],\n",
    "  \"wind_speed\": data[\"wind\"][\"speed\"],\n",
    "  \"weather\": data[\"weather\"][0][\"description\"],\n",
    "  }\n",
    "  \n",
    "  return weather_info\n",
    "\n",
    "lat = 41.38\n",
    "lon = 2.16\n",
    "weather_info = weather(lat, lon)\n",
    "print(weather_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Desafío 14 - Nombre del libro, precio y disponibilidad de stock como un dataframe de pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url14 = 'http://books.toscrape.com/'\n",
    "books = requests.get(url14)\n",
    "soup14 = BeautifulSoup(books.content,\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Titulo': 'A Light in the ...', 'Precio': '£51.77', 'Stock': 'In stock'}, {'Titulo': 'Tipping the Velvet', 'Precio': '£53.74', 'Stock': 'In stock'}, {'Titulo': 'Soumission', 'Precio': '£50.10', 'Stock': 'In stock'}, {'Titulo': 'Sharp Objects', 'Precio': '£47.82', 'Stock': 'In stock'}, {'Titulo': 'Sapiens: A Brief History ...', 'Precio': '£54.23', 'Stock': 'In stock'}, {'Titulo': 'The Requiem Red', 'Precio': '£22.65', 'Stock': 'In stock'}, {'Titulo': 'The Dirty Little Secrets ...', 'Precio': '£33.34', 'Stock': 'In stock'}, {'Titulo': 'The Coming Woman: A ...', 'Precio': '£17.93', 'Stock': 'In stock'}, {'Titulo': 'The Boys in the ...', 'Precio': '£22.60', 'Stock': 'In stock'}, {'Titulo': 'The Black Maria', 'Precio': '£52.15', 'Stock': 'In stock'}, {'Titulo': 'Starving Hearts (Triangular Trade ...', 'Precio': '£13.99', 'Stock': 'In stock'}, {'Titulo': \"Shakespeare's Sonnets\", 'Precio': '£20.66', 'Stock': 'In stock'}, {'Titulo': 'Set Me Free', 'Precio': '£17.46', 'Stock': 'In stock'}, {'Titulo': \"Scott Pilgrim's Precious Little ...\", 'Precio': '£52.29', 'Stock': 'In stock'}, {'Titulo': 'Rip it Up and ...', 'Precio': '£35.02', 'Stock': 'In stock'}, {'Titulo': 'Our Band Could Be ...', 'Precio': '£57.25', 'Stock': 'In stock'}, {'Titulo': 'Olio', 'Precio': '£23.88', 'Stock': 'In stock'}, {'Titulo': 'Mesaerion: The Best Science ...', 'Precio': '£37.59', 'Stock': 'In stock'}, {'Titulo': 'Libertarianism for Beginners', 'Precio': '£51.33', 'Stock': 'In stock'}, {'Titulo': \"It's Only the Himalayas\", 'Precio': '£45.17', 'Stock': 'In stock'}]\n"
     ]
    }
   ],
   "source": [
    "def extraer_libros(soup14, tag, class_name):\n",
    "  libros = []\n",
    "  libros_html = soup14.find_all(tag, class_=class_name)\n",
    "  \n",
    "  for libro_html in libros_html:\n",
    "    libro = libro_html.get_text(strip=True)\n",
    "    libros.append(libro)\n",
    "  \n",
    "  return libros\n",
    "\n",
    "libros_titulos = extraer_libros(soup14, \"h3\", \"\")\n",
    "libros_precios = extraer_libros(soup14, \"p\", \"price_color\")\n",
    "libros_stock = extraer_libros(soup14, \"p\", \"instock\")\n",
    "\n",
    "libros = []\n",
    "for titulo, precio, stock in zip(libros_titulos, libros_precios, libros_stock):\n",
    "  libros.append({'Titulo': titulo, 'Precio': precio, 'Stock': stock})\n",
    "\n",
    "print(libros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Limitates tu output? Gracias! 🙂**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
